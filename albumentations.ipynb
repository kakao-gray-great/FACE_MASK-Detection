{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "albumentations.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsaw6P6IoHTkF3yXqTYAYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oopsys6/FACE_MASK-Detection/blob/main/albumentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CIP-5VeZBXe"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import albumentations\n",
        "import albumentations.pytorch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLcNzMzsfMRC"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGppIG_IfUGm"
      },
      "source": [
        "class TorchvisionDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        file_path = self.file_paths[idx]\n",
        "        \n",
        "        # Read an image with PIL\n",
        "        image = Image.open(file_path)\n",
        "        \n",
        "        start_t = time.time()\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        total_time = (time.time() - start_t)\n",
        "\n",
        "        return image, label, total_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHWlKRtAgKxE"
      },
      "source": [
        "torchvision_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), \n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "torchvision_dataset = TorchvisionDataset(\n",
        "    file_paths=[\"/content/gdrive/My Drive/rabbit.jpg\"],\n",
        "    labels=[1],\n",
        "    transform=torchvision_transform,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzdTEHQ2hLYB"
      },
      "source": [
        "torchvision_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JvJF_PhRNv"
      },
      "source": [
        "torchvision_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-PoTjAYhZ11"
      },
      "source": [
        "total_time = 0\n",
        "for i in range(100):\n",
        "  sample, _, transform_time = torchvision_dataset[0]\n",
        "  total_time += transform_time\n",
        "\n",
        "print(\"torchvision time/sample: {} ms\".format(total_time*10))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(transforms.ToPILImage()(sample))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9lcv0Hhq6M"
      },
      "source": [
        "class AlbumentationsDataset(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        file_path = self.file_paths[idx]\n",
        "        \n",
        "        # Read an image with OpenCV\n",
        "        image = cv2.imread(file_path)\n",
        "        \n",
        "        # By default OpenCV uses BGR color space for color images,\n",
        "        # so we need to convert the image to RGB color space.\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        start_t = time.time()\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "            total_time = (time.time() - start_t)\n",
        "        return image, label, total_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpzhTHK4iN-c"
      },
      "source": [
        "# Same transform with torchvision_transform\n",
        "albumentations_transform = albumentations.Compose([\n",
        "    albumentations.Resize(256, 256), \n",
        "    albumentations.RandomCrop(224, 224),\n",
        "    albumentations.HorizontalFlip(), # Same with transforms.RandomHorizontalFlip()\n",
        "    albumentations.pytorch.transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0swt0onOjayQ"
      },
      "source": [
        "# Same dataset with torchvision_dataset\n",
        "albumentations_dataset = AlbumentationsDataset(\n",
        "    file_paths=[\"/content/gdrive/My Drive/rabbit.jpg\"],\n",
        "    labels=[1],\n",
        "    transform=albumentations_transform,\n",
        ")\n",
        "total_time = 0\n",
        "for i in range(100):\n",
        "    sample, _, transform_time = albumentations_dataset[0]\n",
        "    total_time += transform_time\n",
        "\n",
        "print(\"albumentations time/sample: {} ms\".format(total_time*10))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(transforms.ToPILImage()(sample))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v34Y7uelm8m"
      },
      "source": [
        "albumentations_transform_oneof = albumentations.Compose([\n",
        "    albumentations.Resize(256, 256), \n",
        "    albumentations.RandomCrop(224, 224),\n",
        "    albumentations.OneOf([\n",
        "                          albumentations.HorizontalFlip(p=1),\n",
        "                          albumentations.RandomRotate90(p=1),\n",
        "                          albumentations.VerticalFlip(p=1)            \n",
        "    ], p=1),\n",
        "    albumentations.OneOf([\n",
        "                          albumentations.MotionBlur(p=1),\n",
        "                          albumentations.OpticalDistortion(p=1),\n",
        "                          albumentations.GaussNoise(p=1)                 \n",
        "    ], p=1),\n",
        "    albumentations.pytorch.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euKvdkuNqFJt"
      },
      "source": [
        "albumentations_dataset = AlbumentationsDataset(\n",
        "    file_paths=[\"/content/gdrive/My Drive/rabbit.jpg\"],\n",
        "    labels=[1],\n",
        "    transform=albumentations_transform_oneof,\n",
        ")\n",
        "\n",
        "num_samples = 10\n",
        "fig, ax = plt.subplots(1, num_samples, figsize=(25, 5))\n",
        "for i in range(num_samples):\n",
        "  ax[i].imshow(transforms.ToPILImage()(albumentations_dataset[0][0]))\n",
        "  ax[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KXy6gBJqJz_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}